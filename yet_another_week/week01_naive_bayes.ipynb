{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seminar 01: Naive Bayes from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we will write Naive Bayes classifier supporting different feature probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T07:08:51.904850Z",
     "start_time": "2020-02-11T07:08:50.413258Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First to load dataset we're going to use [`sklearn`](https://scikit-learn.org/stable/) package which we will extensively use during the whole course.\n",
    "\n",
    "`sklearn` implement most of classical and frequently used algorithms in Machine Learning. Also it provides [User Guide](https://scikit-learn.org/stable/user_guide.html) describing principles of every bunch of algorithms implemented.\n",
    "\n",
    "As an entry point to main `sklearn`'s concepts we recommend [getting started tutorial](https://scikit-learn.org/stable/getting_started.html) (check it out yourself). [Further tutorials](https://scikit-learn.org/stable/tutorial/index.html) can also be handy to develop your skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First functionality we use is cosy loading of [common datasets](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets). All we need to do is just one function call.\n",
    "\n",
    "Object generated by [`load_iris`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_iris.html) is described as:\n",
    "\n",
    "> Dictionary-like object, the interesting attributes are:\n",
    ">\n",
    "> ‘data’, the data to learn,\n",
    ">\n",
    ">‘target’, the classification labels,\n",
    ">\n",
    ">‘target_names’, the meaning of the labels,\n",
    ">\n",
    ">‘feature_names’, the meaning of the features,\n",
    ">\n",
    ">‘DESCR’, the full description of the dataset,\n",
    ">\n",
    ">‘filename’, the physical location of iris csv dataset (added in version 0.20)\n",
    "\n",
    "Let's see what we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T07:08:51.918857Z",
     "start_time": "2020-02-11T07:08:51.910566Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = datasets.load_iris()\n",
    "\n",
    "print(dataset.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you aren't familiar with Iris dataset - take a minute to read description above =) (as always [more info about it in Wikipedia](https://en.wikipedia.org/wiki/Iris_flower_data_set))\n",
    "\n",
    "__TL;DR__ 150 objects equally distributed over 3 classes each described with 4 continuous features\n",
    "\n",
    "Just pretty table to look at:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T07:08:51.940271Z",
     "start_time": "2020-02-11T07:08:51.921326Z"
    }
   },
   "outputs": [],
   "source": [
    "# for now you don't need to understand what happens in this code - just look at the table\n",
    "ext_target = dataset.target[:, None]\n",
    "pd.DataFrame(\n",
    "    np.concatenate((dataset.data, ext_target, dataset.target_names[ext_target]), axis=1),\n",
    "    columns=dataset.feature_names + ['target label', 'target name'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now give distinct names to the data we will use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T07:08:52.604007Z",
     "start_time": "2020-02-11T07:08:52.599704Z"
    }
   },
   "outputs": [],
   "source": [
    "features = dataset.data\n",
    "target = dataset.target\n",
    "\n",
    "features.shape, target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Please, remember!!!__\n",
    "\n",
    "Anywhere in our course we have an agreement to shape design matrix (named `features` in code above) as \n",
    "\n",
    "`(#number_of_items, #number_of_features)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset has 4 dimensions however humans are more common to 3 or even 2 dimensional data, so let's plot first 3 features colored with labels values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T07:09:01.519543Z",
     "start_time": "2020-02-11T07:09:01.508622Z"
    }
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T07:09:02.191167Z",
     "start_time": "2020-02-11T07:09:01.870454Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "ax = Axes3D(fig)\n",
    "\n",
    "ax.scatter(features[:, 0], features[:, 1], features[:, 3], c=target, marker='o')\n",
    "ax.set_xlabel(dataset.feature_names[0])\n",
    "ax.set_ylabel(dataset.feature_names[1])\n",
    "ax.set_zlabel(dataset.feature_names[2])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then have a look on feature distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T07:09:03.615686Z",
     "start_time": "2020-02-11T07:09:02.922717Z"
    }
   },
   "outputs": [],
   "source": [
    "# remember this way to make subplots! It could be useful for you later in your work\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "for i, axis in enumerate(axes.flat):\n",
    "    axis.hist(features[:, i])\n",
    "    axis.set_xlabel(dataset.feature_names[i])\n",
    "    axis.set_ylabel('number of objects')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that every plot above have own scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we aiming to implement Naive Bayes algorithm first we need some prior distribution defined.\n",
    "\n",
    "The most common distribution is (of course) Gaussian and it's params are mean and standard deviation. Let's implement class taking list of feature values, estimating distribution params and able to give probability density of any given feature value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T07:09:05.944368Z",
     "start_time": "2020-02-11T07:09:05.938558Z"
    }
   },
   "outputs": [],
   "source": [
    "class GaussianDistribution:\n",
    "    def __init__(self, feature):\n",
    "        '''\n",
    "        Args:\n",
    "            feature: column of design matrix, represents all available values\n",
    "                of feature to model\n",
    "        '''\n",
    "        self.mean = feature.mean()\n",
    "        self.std = feature.std()\n",
    "\n",
    "    def log_proba(self, value):\n",
    "        '''Logarithm of probability density at value'''\n",
    "        return # <YOUR CODE HERE>\n",
    "    \n",
    "    def proba(self, value):\n",
    "        return # <YOUR CODE HERE>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T07:09:06.710243Z",
     "start_time": "2020-02-11T07:09:06.706347Z"
    }
   },
   "outputs": [],
   "source": [
    "assert np.allclose(\n",
    "    GaussianDistribution(features[:, 2]).proba(features[:5, 2]),\n",
    "    np.array([0.19195815, 0.19195815, 0.18463525, 0.19924939, 0.19195815])\n",
    "), 'Something wrong with the GaussianDistribution class'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to implement classifier itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T00:09:57.874598Z",
     "start_time": "2020-02-11T00:09:57.740553Z"
    }
   },
   "source": [
    "![title](https://www.saedsayad.com/images/Bayes_rule.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T07:30:45.357053Z",
     "start_time": "2020-02-11T07:30:45.346489Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.special import logsumexp\n",
    "\n",
    "\n",
    "class NaiveBayes():\n",
    "    def fit(self, data, labels, distributions=None):\n",
    "        self.unique_labels = np.unique(labels)\n",
    "        \n",
    "        distributions = distributions or [GaussianDistribution] * data.shape[1]\n",
    "        self.label_likelihood = {}\n",
    "        for label in self.unique_labels:\n",
    "            distr_for_column = []\n",
    "            for column_index in range(data.shape[1]):\n",
    "                feature_column = data[labels == label, column_index]\n",
    "                distr = distributions[column_index](feature_column)\n",
    "                distr_for_column.append(distr)\n",
    "            self.label_likelihood[label] = distr_for_column\n",
    "\n",
    "        self.label_prior = {\n",
    "            # <YOUR CODE HERE>\n",
    "        }\n",
    "\n",
    "    def predict_log_proba(self, batch):\n",
    "        class_log_probas = np.zeros((batch.shape[0], len(self.unique_labels)))\n",
    "        for label_idx, label in enumerate(self.unique_labels):\n",
    "            for idx in range(batch.shape[1]):\n",
    "                # All loglikelihood for every feature w.r.t. fixed label\n",
    "                class_log_probas[:, label_idx] += # <YOUR CODE HERE>\n",
    "            # Add log proba of label prior\n",
    "            class_log_probas[:, label_idx] += # <YOUR CODE HERE>\n",
    "\n",
    "        for idx in range(batch.shape[1]):\n",
    "        # If you want to get probabilities, you need to substract the log proba for every feature\n",
    "            class_log_probas -= # <YOUR CODE HERE>\n",
    "        return class_log_probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T07:30:45.713112Z",
     "start_time": "2020-02-11T07:30:45.709195Z"
    }
   },
   "outputs": [],
   "source": [
    "nb = NaiveBayes()\n",
    "nb.fit(features, target, distributions=[GaussianDistribution]*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T07:30:45.991283Z",
     "start_time": "2020-02-11T07:30:45.978941Z"
    }
   },
   "outputs": [],
   "source": [
    "nb_proba = nb.predict_log_proba(features)\n",
    "nb_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare with reference implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T07:30:47.513267Z",
     "start_time": "2020-02-11T07:30:47.498843Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "external_nb = GaussianNB()\n",
    "\n",
    "external_nb.fit(features, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T07:30:47.853360Z",
     "start_time": "2020-02-11T07:30:47.841855Z"
    }
   },
   "outputs": [],
   "source": [
    "ext_nb_proba = external_nb.predict_proba(features)\n",
    "ext_nb_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T07:30:48.319855Z",
     "start_time": "2020-02-11T07:30:48.309685Z"
    }
   },
   "outputs": [],
   "source": [
    "nb_proba - ext_nb_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced distribution for NaiveBayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we do love Gaussian distribution it is still unimodal while our features are substantially multimodal (see histograms above). So we have to implement more robust distribution estimator - Kernel Density Estimator (KDE).\n",
    "\n",
    "Idea for this method is simple: we assign some probability density to a region around actual observation. (We will return to density estimation methods to describe them carefully later in this course).\n",
    "\n",
    "Fortunately `sklearn` have KDE implemented for us already. All it needs is vector of feature values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T00:27:57.223204Z",
     "start_time": "2020-02-11T00:27:54.891Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T00:27:57.225367Z",
     "start_time": "2020-02-11T00:27:54.893Z"
    }
   },
   "outputs": [],
   "source": [
    "kde = KernelDensity(kernel='gaussian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T00:27:57.230835Z",
     "start_time": "2020-02-11T00:27:54.895Z"
    }
   },
   "outputs": [],
   "source": [
    "kde.fit(features[:, 2].reshape((-1, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T00:27:57.237856Z",
     "start_time": "2020-02-11T00:27:54.896Z"
    }
   },
   "outputs": [],
   "source": [
    "class GaussianKDE:\n",
    "    def __init__(self, feature):\n",
    "        self.kde = KernelDensity()\n",
    "        self.kde.fit(feature.reshape((-1, 1)))\n",
    "\n",
    "    def log_proba(self, value):\n",
    "        return self.kde.score_samples(value.reshape((-1, 1)))\n",
    "\n",
    "    def proba(self, value):\n",
    "        return np.exp(self.log_proba(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T00:27:57.240607Z",
     "start_time": "2020-02-11T00:27:54.898Z"
    }
   },
   "outputs": [],
   "source": [
    "a = GaussianKDE(features[:, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-11T00:27:57.242344Z",
     "start_time": "2020-02-11T00:27:54.900Z"
    }
   },
   "outputs": [],
   "source": [
    "a.proba(features[:5, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare the classifiers using number of errors ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
